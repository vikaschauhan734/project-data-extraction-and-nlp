{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Excel Sheet to Pandas DataFrame\n",
    "df = pd.read_csv('Input.xlsx - Sheet1.csv')\n",
    "\n",
    "\n",
    "URL = df.loc[:,\"URL\"]\n",
    "#print(URL)\n",
    "URL_ID = df.loc[:,\"URL_ID\"]\n",
    "#print(URL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in DataFrame\n",
    "columns=[\n",
    "    \"POSITIVE SCORE\",\n",
    "    \"NEGATIVE SCORE\",\n",
    "    \"POLARITY SCORE\",\n",
    "    \"SUBJECTIVITY SCORE\",\n",
    "    \"AVG SENTENCE LENGTH\",\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\",\n",
    "    \"FOG INDEX\",\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
    "    \"COMPLEX WORD COUNT\",\n",
    "    \"WORD COUNT\",\n",
    "    \"SYLLABLE PER WORD\",\n",
    "    \"AVG SYLLABLE COUNT PER WORD\",\n",
    "    \"PERSONAL PRONOUNS\",\n",
    "    \"AVG WORD LENGTH\"\n",
    "]\n",
    "for c in columns:\n",
    "    df.insert(columns.index(c)+2 ,c, value=None)\n",
    "\n",
    "# Print the DataFrame\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported stopWords\n",
    "import os\n",
    "\n",
    "directory = \"D:\\projects\\data_extraction_and_NLP\\StopWords\"\n",
    "text_files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "\n",
    "stop_words = []\n",
    "for filename in text_files:\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        contents = file.read().replace(\"\\n\", \" \")\n",
    "    stop_words.append(contents)\n",
    "\n",
    "#print(stop_words[0])\n",
    "#print(type(stop_words))\n",
    "\n",
    "#Tokenize stopwords\n",
    "swords = word_tokenize(str(stop_words))\n",
    "swords = text_without_symbols = [word for word in swords if word.isalpha()]\n",
    "#print(swords)\n",
    "#print(type(swords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported positive-words.txt\n",
    "filename = \"positive-words.txt\"\n",
    "directory = \"D:\\projects\\data_extraction_and_NLP\\MasterDictionary\"\n",
    "\n",
    "with open(os.path.join(directory, filename), \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    contents = file.read().replace(\"\\n\", \" \")\n",
    "positive_words = contents\n",
    "\n",
    "#print(positive_words)\n",
    "#print(type(positive_words))\n",
    "\n",
    "#Tokenize positivewords\n",
    "pwords = word_tokenize(str(positive_words))\n",
    "#print(pwords)\n",
    "#print(type(pwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported negative-words.txt\n",
    "filename = \"negative-words.txt\"\n",
    "directory = \"D:\\projects\\data_extraction_and_NLP\\MasterDictionary\"\n",
    "with open(os.path.join(directory, filename), \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    contents = file.read().replace(\"\\n\", \" \")\n",
    "negative_words = contents\n",
    "\n",
    "#print(negative_words)\n",
    "#print(type(negative_words))\n",
    "\n",
    "#Tokenize negativewords\n",
    "nwords = word_tokenize(str(negative_words))\n",
    "#print(nwords)\n",
    "#print(type(nwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,114):\n",
    "    #Naming filename as URL_ID\n",
    "    filename = str(URL_ID[i]) + \".txt\"\n",
    "    url = URL[i]\n",
    "    resp = requests.get(url)\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    #Saving Title as string\n",
    "    for header in soup.find_all('h1'):\n",
    "        title = str(header.text)\n",
    "    #Saving Article text as string\n",
    "    article_text = \"\"\n",
    "    for article in soup.find_all('p'):\n",
    "        info = article.text\n",
    "        article_text = article_text + \" \" + info\n",
    "    #Appending Title and Article text\n",
    "    complete = title + article_text\n",
    "    #Creating file and saving complete article in file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(str(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,114):\n",
    "    #Naming filename as URL_ID\n",
    "    filename = str(URL_ID[i]) + \".txt\"\n",
    "    url = URL[i]\n",
    "    resp = requests.get(url)\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    #Saving Title as string\n",
    "    for header in soup.find_all('h1'):\n",
    "        title = str(header.text)\n",
    "    #Saving Article text as string\n",
    "    article_text = \"\"\n",
    "    for article in soup.find_all('p'):\n",
    "        info = article.text\n",
    "        article_text = article_text + \" \" + info\n",
    "    #Appending Title and Article text\n",
    "    complete = title + article_text\n",
    "    #Creating file and saving complete article in file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(str(complete))\n",
    "    \n",
    "    ##########################################################################################\n",
    "\n",
    "    #Imported text file\n",
    "    with open(filename, \"r\") as file:\n",
    "        contents = file.read()\n",
    "    text = contents\n",
    "    #print(type(text))\n",
    "    \n",
    "    ###########################################################################################\n",
    "\n",
    "    #Tokenized the text to words\n",
    "    words = word_tokenize(text)\n",
    "    #Removed all symbols from text\n",
    "    text_without_symbols = [word for word in words if word.isalpha()]\n",
    "    #Tokens without stopwords\n",
    "    cleaned_text = [x for x in text_without_symbols if x not in swords]\n",
    "    #print(cleaned_text)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    #Positive Score\n",
    "    posscore = 0\n",
    "    for x in cleaned_text:\n",
    "       if x in pwords:\n",
    "            posscore += 1\n",
    "    #print(posscore)\n",
    "\n",
    "    #############################################################################################\n",
    "\n",
    "    #Negative Score\n",
    "    negscore = 0\n",
    "    for x in cleaned_text:\n",
    "       if x in nwords:\n",
    "            negscore += 1\n",
    "    #print(negscore)\n",
    "\n",
    "    ##############################################################################################\n",
    "\n",
    "    #Polarity Score\n",
    "    polarityscore = (posscore - negscore)/((posscore + negscore) + 0.000001)\n",
    "    #print(polarityscore)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Subjectivity Score\n",
    "    subscore = (posscore + negscore)/ (len(cleaned_text) + 0.000001)\n",
    "    #print(subscore)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Tokenized the text to sentences\n",
    "    sent = sent_tokenize(text)\n",
    "    #print(sent)\n",
    "    #print(len(sent))\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Average Sentence Length = the number of words / the number of sentences\n",
    "    avg_sent_length = len(text_without_symbols)/len(sent)\n",
    "    #print(avg_sent_length)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Complex Words\n",
    "    #nltk.download('cmudict')\n",
    "    d = cmudict.dict()\n",
    "\n",
    "    def count_syllables(word):\n",
    "        try:\n",
    "            return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    complex_words = []\n",
    "    for word in cleaned_text:\n",
    "        if count_syllables(word) > 2:\n",
    "            complex_words.append(word)\n",
    "    #print(complex_words)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Percentage of Complex words = the number of complex words / the number of words \n",
    "    percwords = len(complex_words)/len(cleaned_text)\n",
    "    #print(percwords)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "    fogindex = 0.4*(avg_sent_length + percwords)\n",
    "    #print(fogindex)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "    avg_no_of_words_sent = len(text_without_symbols)/len(sent)\n",
    "    #print(avg_no_of_words_sent)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Complex Word Count\n",
    "    cwordcount = len(complex_words)\n",
    "    #print(cwordcount)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Word Count\n",
    "    wordcount = len(cleaned_text)\n",
    "    #print(wordcount)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Syllable Count Per Word\n",
    "    def count_syllables(word):\n",
    "        vowels = \"aeiouy\"\n",
    "        word = word.lower().strip(\".:;?!\")\n",
    "        if word[-2:] == \"ed\" or word[-2:] == \"es\":\n",
    "            word = word[:-2]\n",
    "        if word[-1:] == \"e\":\n",
    "            word = word[:-1]\n",
    "        if word[-3:] == \"ing\":\n",
    "            word = word[:-3]\n",
    "        count = 0\n",
    "        for index, char in enumerate(word):\n",
    "            if char in vowels and (index == 0 or word[index-1] not in vowels):\n",
    "                count += 1\n",
    "        if word[-1] in vowels:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    def syllable_count_per_word(cleaned_text):\n",
    "        syllable_counts = [count_syllables(word) for word in cleaned_text]\n",
    "        return dict(zip(cleaned_text, syllable_counts))\n",
    "\n",
    "    syllable_per_word = syllable_count_per_word(cleaned_text)\n",
    "    #print(syllable_per_word)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Total Syllables\n",
    "    def total_syllables(cleaned_text):\n",
    "        syllable_counts = [count_syllables(word) for word in cleaned_text]\n",
    "        total_syllable = sum(syllable_counts)\n",
    "        return total_syllable\n",
    "\n",
    "    total_syllable = total_syllables(cleaned_text)\n",
    "    #print(total_syllable)\n",
    "\n",
    "\n",
    "    #Average Syllable Count Per Word\n",
    "    avg_syllable_count_per_word = total_syllable/len(cleaned_text)\n",
    "    #print(avg_syllable_count_per_word)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Personal Pronouns\n",
    "    def count_personal_pronouns(cleaned_text):\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = 0\n",
    "        for word in cleaned_text:\n",
    "            if word in personal_pronouns:\n",
    "                count += 1\n",
    "            elif word == \"US\":\n",
    "                count -= 1\n",
    "        return count\n",
    "\n",
    "    personal_pronouns = count_personal_pronouns(cleaned_text)\n",
    "    if personal_pronouns < 0:\n",
    "        personal_pronouns = 0\n",
    "    #print(personal_pronouns)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    #Average Word Length = Sum of the total number of characters in each word/Total number of words\n",
    "    def sum_of_characters(cleaned_text):\n",
    "        total_sum = 0\n",
    "        for word in cleaned_text:\n",
    "            total_sum += len(word)\n",
    "        return total_sum\n",
    "    total_characters = sum_of_characters(cleaned_text)\n",
    "    #print(total_characters)\n",
    "\n",
    "    avg_word_length = total_characters/len(cleaned_text)\n",
    "    #print(avg_word_length)\n",
    "\n",
    "    ##############################################################################################\n",
    "    \n",
    "    # Add data to the DataFrame\n",
    "    df.loc[i] = [URL_ID[i], URL[i], posscore, negscore, polarityscore, subscore, avg_sent_length, percwords, fogindex, avg_no_of_words_sent, cwordcount, wordcount, syllable_per_word, avg_syllable_count_per_word, personal_pronouns, avg_word_length]\n",
    "\n",
    "    # Print the DataFrame\n",
    "    #print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   URL_ID                                                URL POSITIVE SCORE  \\\n",
      "0      37  https://insights.blackcoffer.com/ai-in-healthc...             66   \n",
      "1      38  https://insights.blackcoffer.com/what-if-the-c...             59   \n",
      "2      39  https://insights.blackcoffer.com/what-jobs-wil...             68   \n",
      "3      40  https://insights.blackcoffer.com/will-machine-...             59   \n",
      "4      41  https://insights.blackcoffer.com/will-ai-repla...             50   \n",
      "\n",
      "  NEGATIVE SCORE POLARITY SCORE SUBJECTIVITY SCORE AVG SENTENCE LENGTH  \\\n",
      "0             33       0.333333           0.088078             24.0625   \n",
      "1             37       0.229167           0.128859           18.552941   \n",
      "2             33       0.346535           0.098058           20.744444   \n",
      "3             24       0.421687           0.100484           18.030928   \n",
      "4             22       0.388889           0.076923           22.439024   \n",
      "\n",
      "  PERCENTAGE OF COMPLEX WORDS FOG INDEX AVG NUMBER OF WORDS PER SENTENCE  \\\n",
      "0                    0.368327  9.772331                          24.0625   \n",
      "1                    0.302013  7.541982                        18.552941   \n",
      "2                    0.366019  8.444186                        20.744444   \n",
      "3                    0.311138  7.336826                        18.030928   \n",
      "4                    0.342949  9.112789                        22.439024   \n",
      "\n",
      "  COMPLEX WORD COUNT WORD COUNT  \\\n",
      "0                414       1124   \n",
      "1                225        745   \n",
      "2                377       1030   \n",
      "3                257        826   \n",
      "4                321        936   \n",
      "\n",
      "                                   SYLLABLE PER WORD  \\\n",
      "0  {'healthcare': 2, 'Improve': 2, 'Patient': 2, ...   \n",
      "1  {'What': 1, 'Creation': 2, 'Taking': 1, 'Over'...   \n",
      "2  {'What': 1, 'Jobs': 1, 'Will': 1, 'Robots': 2,...   \n",
      "3  {'Will': 1, 'Machine': 2, 'Replace': 2, 'The':...   \n",
      "4  {'Will': 1, 'Replace': 2, 'Us': 1, 'Work': 1, ...   \n",
      "\n",
      "  AVG SYLLABLE COUNT PER WORD PERSONAL PRONOUNS AVG WORD LENGTH  \n",
      "0                    2.325623                 0        7.293594  \n",
      "1                    2.104698                 0        6.708725  \n",
      "2                    2.296117                 0         7.23301  \n",
      "3                    2.136804                 0        6.693705  \n",
      "4                    2.192308                 0        6.866453  \n"
     ]
    }
   ],
   "source": [
    "#First five rows\n",
    "print(df.head(5))\n",
    "\n",
    "#Saving dataframe to excel file\n",
    "df.to_excel('Output Data Structure.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
